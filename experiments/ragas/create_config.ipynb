{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zy/dvp0hbvx6x7bkqw0xgf9skg00000gn/T/ipykernel_62581/71235559.py:9: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# you can check out the the documentation for the rest of the autoreaload modes\n",
    "# by apending a question mark to %autoreload, like this:\n",
    "# %autoreload?\n",
    "\n",
    "\n",
    "from IPython.lib import deepreload\n",
    "import imp\n",
    "def my_reload(modules=[], deep=False):\n",
    "    if not isinstance(modules, list):\n",
    "        modules = [modules]\n",
    "    if deep:\n",
    "        rl_func = deepreload.reload\n",
    "    else:\n",
    "        rl_func = imp.reload\n",
    "    \n",
    "    for m in modules:\n",
    "        try:\n",
    "            rl_func(m)\n",
    "        except:\n",
    "            print(f\"THIS IS A COMMON ISSUE: some modules in {m} are not work\")\n",
    "\n",
    "import sys, os, yaml\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0,\"/Users/bill/Projects/Pycharm_projects/AutoRAG\")\n",
    "config_path = \"/Users/bill/Projects/Pycharm_projects/AutoRAG/ragas/configuration/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.configuration import RetrievalConfiguration,VectorDBConfiguration,GeneratorConfiguration\n",
    "\n",
    "ModuleConfig = VectorDBConfiguration.load_from_yaml(config_path, key=\"vectordb\")\n",
    "\n",
    "# config_path = \"/Users/bill/Projects/Pycharm_projects/AutoRAG/ragas/configuration/config.yaml\"\n",
    "# retriever = RetrievalConfiguration.load_from_yaml(config_path, key=\"retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Configuration space object:\n",
       "  Hyperparameters:\n",
       "vectordb\n",
       "    collection_name, Type: Categorical, Choices: {huggingface_all_mpnet_base_v2}, Default: huggingface_all_mpnet_base_v2\n",
       "    db_type, Type: Categorical, Choices: {chroma}, Default: chroma\n",
       "    embedding_batch, Type: Categorical, Choices: {100, 200}, Default: 100\n",
       "    embedding_model, Type: Categorical, Choices: {huggingface_all_mpnet_base_v2}, Default: huggingface_all_mpnet_base_v2\n",
       "    similarity_metric, Type: Categorical, Choices: {cosine, euclidean, ip}, Default: cosine"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModuleConfig.cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'default_vectordb',\n",
       "  'collection_name': 'huggingface_all_mpnet_base_v2',\n",
       "  'db_type': 'chroma',\n",
       "  'embedding_batch': 200,\n",
       "  'embedding_model': 'huggingface_all_mpnet_base_v2',\n",
       "  'similarity_metric': 'cosine',\n",
       "  'client_type': 'persistent',\n",
       "  'host': 'localhost',\n",
       "  'port': 8000,\n",
       "  'ssl': False,\n",
       "  'headers': None,\n",
       "  'api_key': None,\n",
       "  'tanant': 'default_tenant',\n",
       "  'database': 'default_database',\n",
       "  'path': '/home/lyb/RAG/experiments/chroma_mpnet'},\n",
       " {'name': 'default_vectordb',\n",
       "  'collection_name': 'huggingface_all_mpnet_base_v2',\n",
       "  'db_type': 'chroma',\n",
       "  'embedding_batch': 100,\n",
       "  'embedding_model': 'huggingface_all_mpnet_base_v2',\n",
       "  'similarity_metric': 'cosine',\n",
       "  'client_type': 'persistent',\n",
       "  'host': 'localhost',\n",
       "  'port': 8000,\n",
       "  'ssl': False,\n",
       "  'headers': None,\n",
       "  'api_key': None,\n",
       "  'tanant': 'default_tenant',\n",
       "  'database': 'default_database',\n",
       "  'path': '/home/lyb/RAG/experiments/chroma_mpnet'},\n",
       " {'name': 'default_vectordb',\n",
       "  'collection_name': 'huggingface_all_mpnet_base_v2',\n",
       "  'db_type': 'chroma',\n",
       "  'embedding_batch': 200,\n",
       "  'embedding_model': 'huggingface_all_mpnet_base_v2',\n",
       "  'similarity_metric': 'euclidean',\n",
       "  'client_type': 'persistent',\n",
       "  'host': 'localhost',\n",
       "  'port': 8000,\n",
       "  'ssl': False,\n",
       "  'headers': None,\n",
       "  'api_key': None,\n",
       "  'tanant': 'default_tenant',\n",
       "  'database': 'default_database',\n",
       "  'path': '/home/lyb/RAG/experiments/chroma_mpnet'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModuleConfig.create_nodes(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': ['hybrid_rrf'], 'top_k': [5, 10], 'bm25': {'bm25_tokenizer': ['porter_stemmer', 'space']}, 'vectordb': {'embedding_batch': [256]}, 'hybrid_cc': {'normalize_method': ['tmm', 'mm', 'z', 'dbsf'], 'weight': (0.0, 1.0), 'lexical_theoretical_min_value': 0, 'semantic_theoretical_min_value': -1}, 'hybrid_rrf': {'weight': (4, 80)}}\n",
      "{'method': ['chroma'], 'collection_name': ['huggingface_all_mpnet_base_v2'], 'embedding_model': ['huggingface_all_mpnet_base_v2'], 'embedding_batch': [100, 200], 'similarity_metric': ['cosine', 'l2', 'ip'], 'couchbase': {'ingest_batch': [100, 200]}, 'pinecone': {'ingest_batch': [100, 200]}, 'milvus': {'ingest_batch': [100, 200], 'index_type': ['IVF_FLAT', 'ScaNN', 'IVF_SQ8', 'SCANN', 'GPU_CAGRA', 'GPU_IVF_FLAT']}}\n",
      "{'method': ['LlamaIndexLLM'], 'temperature': [0.1, 0.5], 'batch': [1], 'max_token': 512, 'LlamaIndexLLM': {'llm': ['ollama'], 'model': ['deepseek-r1:70b'], 'torch_dtype': 'float16'}, 'OpenAILLM': {'llm': ['gpt-3.5-turbo', 'gpt-4-turbo-2024-04-09']}, 'Vllm': {'llm': ['mistralai/Mistral-7B-Instruct-v0.2', 'facebook/opt-125m'], 'top_p': (0, 1.0)}, 'VllmAPI': {'llm': ['Qwen/Qwen2.5-14B-Instruct-AWQ']}}\n",
      "{'method': ['fstring', 'window_replacement', 'long_context_reorder'], 'prompt': ['Tell me something about the question: {query} \\n {retrieved_contents}', \"Question: {query} \\n Something to read: {retrieved_contents} \\n What's your answer?\"]}\n",
      "({'name': 'default_vectordb', 'collection_name': 'huggingface_all_mpnet_base_v2', 'db_type': 'chroma', 'embedding_batch': 200, 'embedding_model': 'huggingface_all_mpnet_base_v2', 'similarity_metric': 'cosine', 'client_type': 'persistent', 'host': 'localhost', 'port': 8000, 'ssl': False, 'headers': None, 'api_key': None, 'tenant': 'default_tenant', 'database': 'default_database', 'path': '/home/lyb/RAG/experiments/default_vectordb'}, {'node_line_name': 'retrieve_node_line', 'nodes': [{'strategy': {'metrics': ['retrieval_f1', 'retrieval_recall', 'retrieval_precision']}, 'node_type': 'retrieval', 'modules': [{'module_type': 'hybrid_rrf', 'top_k': 10, 'weight': 12, 'target_modules': \"('bm25', 'vectordb')\", 'target_module_params': [{'top_k': 10}, {'top_k': 10, 'vectordb': 'default_vectordb'}]}]}]}, {'node_line_name': 'post_retrieve_node_line', 'nodes': [{'node_type': 'prompt_maker', 'strategy': {'metrics': ['meteor', 'rouge', 'bert_score']}, 'modules': [{'module_type': 'fstring', 'prompt': \"Question: {query} \\n Something to read: {retrieved_contents} \\n What's your answer?\"}]}, {'node_type': 'generator', 'strategy': {'metrics': ['meteor', 'rouge', 'bert_score']}, 'modules': [{'batch': 1, 'max_token': 512, 'module_type': 'LlamaIndexLLM', 'temperature': 0.1, 'llm': 'ollama', 'model': 'deepseek-r1:70b', 'torch_dtype': 'float16', 'api_base': 'your_api_base', 'api_key': 'your_api_key', 'request_timeout': 120.0}]}]})\n",
      "({'name': 'default_vectordb', 'collection_name': 'huggingface_all_mpnet_base_v2', 'db_type': 'chroma', 'embedding_batch': 200, 'embedding_model': 'huggingface_all_mpnet_base_v2', 'similarity_metric': 'l2', 'client_type': 'persistent', 'host': 'localhost', 'port': 8000, 'ssl': False, 'headers': None, 'api_key': None, 'tenant': 'default_tenant', 'database': 'default_database', 'path': '/home/lyb/RAG/experiments/default_vectordb'}, {'node_line_name': 'retrieve_node_line', 'nodes': [{'strategy': {'metrics': ['retrieval_f1', 'retrieval_recall', 'retrieval_precision']}, 'node_type': 'retrieval', 'modules': [{'module_type': 'hybrid_rrf', 'top_k': 5, 'weight': 8, 'target_modules': \"('bm25', 'vectordb')\", 'target_module_params': [{'top_k': 5}, {'top_k': 5, 'vectordb': 'default_vectordb'}]}]}]}, {'node_line_name': 'post_retrieve_node_line', 'nodes': [{'node_type': 'prompt_maker', 'strategy': {'metrics': ['meteor', 'rouge', 'bert_score']}, 'modules': [{'module_type': 'long_context_reorder', 'prompt': \"Question: {query} \\n Something to read: {retrieved_contents} \\n What's your answer?\"}]}, {'node_type': 'generator', 'strategy': {'metrics': ['meteor', 'rouge', 'bert_score']}, 'modules': [{'batch': 1, 'max_token': 512, 'module_type': 'LlamaIndexLLM', 'temperature': 0.5, 'llm': 'ollama', 'model': 'deepseek-r1:70b', 'torch_dtype': 'float16', 'api_base': 'your_api_base', 'api_key': 'your_api_key', 'request_timeout': 120.0}]}]})\n",
      "({'name': 'default_vectordb', 'collection_name': 'huggingface_all_mpnet_base_v2', 'db_type': 'chroma', 'embedding_batch': 200, 'embedding_model': 'huggingface_all_mpnet_base_v2', 'similarity_metric': 'ip', 'client_type': 'persistent', 'host': 'localhost', 'port': 8000, 'ssl': False, 'headers': None, 'api_key': None, 'tenant': 'default_tenant', 'database': 'default_database', 'path': '/home/lyb/RAG/experiments/default_vectordb'}, {'node_line_name': 'retrieve_node_line', 'nodes': [{'strategy': {'metrics': ['retrieval_f1', 'retrieval_recall', 'retrieval_precision']}, 'node_type': 'retrieval', 'modules': [{'module_type': 'hybrid_rrf', 'top_k': 5, 'weight': 75, 'target_modules': \"('bm25', 'vectordb')\", 'target_module_params': [{'top_k': 5}, {'top_k': 5, 'vectordb': 'default_vectordb'}]}]}]}, {'node_line_name': 'post_retrieve_node_line', 'nodes': [{'node_type': 'prompt_maker', 'strategy': {'metrics': ['meteor', 'rouge', 'bert_score']}, 'modules': [{'module_type': 'fstring', 'prompt': 'Tell me something about the question: {query} \\n {retrieved_contents}'}]}, {'node_type': 'generator', 'strategy': {'metrics': ['meteor', 'rouge', 'bert_score']}, 'modules': [{'batch': 1, 'max_token': 512, 'module_type': 'LlamaIndexLLM', 'temperature': 0.5, 'llm': 'ollama', 'model': 'deepseek-r1:70b', 'torch_dtype': 'float16', 'api_base': 'your_api_base', 'api_key': 'your_api_key', 'request_timeout': 120.0}]}]})\n"
     ]
    }
   ],
   "source": [
    "from ragas.configuration import *\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "\n",
    "size = 3\n",
    "retriever = RetrievalConfiguration.load_from_yaml(config_path, key=\"retrieval\")\n",
    "retriever_node_lines = retriever.create_node_lines(size=size)\n",
    "\n",
    "vectordb = VectorDBConfiguration.load_from_yaml(config_path, key=\"vectordb\")\n",
    "vectordb_node_lines = vectordb.create_node_lines(size=size)\n",
    "\n",
    "generator = GeneratorConfiguration.load_from_yaml(config_path, key=\"generator\")\n",
    "generator_nodes = generator.create_nodes(size=size)\n",
    "\n",
    "prompt_maker = PromptMakerConfiguration.load_from_yaml(config_path, key=\"prompt_maker\")\n",
    "prompt_maker_nodes = prompt_maker.create_nodes(size=size)\n",
    "\n",
    "post_retrieve_node_lines = create_lines_with_nodes(\"post_retrieve_node_line\", prompt_maker_nodes, generator_nodes)\n",
    "\n",
    "zipped = zip(vectordb_node_lines, retriever_node_lines, post_retrieve_node_lines)\n",
    "for i, node_line in enumerate(zipped):\n",
    "    vectordb_node_line, retriever_node_line, generator_node_line = node_line\n",
    "    print(node_line)\n",
    "\n",
    "    data = {\n",
    "        'vectordb': [\n",
    "            vectordb_node_line\n",
    "        ],\n",
    "        'node_lines': [\n",
    "            retriever_node_line,\n",
    "            generator_node_line\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Save the modified data back to a YAML file\n",
    "    with open(f\"../candidate_config/ollama_config_{i}.yaml\", \"w\") as file:\n",
    "        yaml.safe_dump(data, file, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_steps_dict is None or doesn't contain the number of points to divide [hybrid_cc]weight into. And its quantization factor is None. Please provide/set one of these values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mActiveHyperparameterNotSetError\u001b[0m           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.10/site-packages/ConfigSpace/util.py:779\u001b[0m, in \u001b[0;36mgenerate_grid\u001b[0;34m(configuration_space, num_steps_dict)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 779\u001b[0m     grid_point \u001b[38;5;241m=\u001b[39m \u001b[43mConfiguration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munchecked_grid_pts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m     checked_grid_pts\u001b[38;5;241m.\u001b[39mappend(grid_point)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.10/site-packages/ConfigSpace/configuration.py:126\u001b[0m, in \u001b[0;36mConfiguration.__init__\u001b[0;34m(self, configuration_space, values, vector, allow_inactive_with_values, origin, config_id)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector[i] \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mto_vector(value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_valid_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m vector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.10/site-packages/ConfigSpace/configuration.py:160\u001b[0m, in \u001b[0;36mConfiguration.check_valid_configuration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mConfigSpace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_configuration\n\u001b[0;32m--> 160\u001b[0m \u001b[43mcheck_configuration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_inactive_with_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_inactive_with_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.10/site-packages/ConfigSpace/util.py:593\u001b[0m, in \u001b[0;36mcheck_configuration\u001b[0;34m(space, vector, allow_inactive_with_values)\u001b[0m\n\u001b[1;32m    592\u001b[0m     hp \u001b[38;5;241m=\u001b[39m space[hp_name]\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ActiveHyperparameterNotSetError(hp)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hp_idx, hp_node \u001b[38;5;129;01min\u001b[39;00m cnode\u001b[38;5;241m.\u001b[39munique_children\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# OPTIM: We bypass the larger safety checking of the hp and access\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# the underlying transformer directly\u001b[39;00m\n",
      "\u001b[0;31mActiveHyperparameterNotSetError\u001b[0m: Hyperparameter is active but has no value set.\n[hybrid_cc]lexical_theoretical_min_value, Type: Constant, Value: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mConfigSpace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_grid\n\u001b[0;32m----> 3\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.10/site-packages/ConfigSpace/util.py:817\u001b[0m, in \u001b[0;36mgenerate_grid\u001b[0;34m(configuration_space, num_steps_dict)\u001b[0m\n\u001b[1;32m    814\u001b[0m                 new_active_hp_names\u001b[38;5;241m.\u001b[39mappend(new_hp_name)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hp_name \u001b[38;5;129;01min\u001b[39;00m new_active_hp_names:\n\u001b[0;32m--> 817\u001b[0m     value_sets\u001b[38;5;241m.\u001b[39mappend(\u001b[43m_get_value_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_steps_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp_name\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    818\u001b[0m     hp_names\u001b[38;5;241m.\u001b[39mappend(hp_name)\n\u001b[1;32m    820\u001b[0m \u001b[38;5;66;03m# this check might not be needed, as there is always going to be a new\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# active HP when in this except block?\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.10/site-packages/ConfigSpace/util.py:695\u001b[0m, in \u001b[0;36mgenerate_grid.<locals>._get_value_set\u001b[0;34m(num_steps_dict, hp_name)\u001b[0m\n\u001b[1;32m    693\u001b[0m     grid_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(lower, upper, num_steps)\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_steps_dict is None or doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain the number of points\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to divide \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into. And its quantization factor \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis None. Please provide/set one of these values.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    699\u001b[0m     )\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mlog:\n\u001b[1;32m    702\u001b[0m     grid_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(grid_points)\n",
      "\u001b[0;31mValueError\u001b[0m: num_steps_dict is None or doesn't contain the number of points to divide [hybrid_cc]weight into. And its quantization factor is None. Please provide/set one of these values."
     ]
    }
   ],
   "source": [
    "from ConfigSpace.util import generate_grid\n",
    "\n",
    "s = generate_grid(retriever.cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.cs.get('[bm25]bm25_tokenizer') == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(\n",
    "            {\n",
    "                \"filename\": [1],\n",
    "                \"execution_time\": [2],\n",
    "                \"average_output_token\": [3],\n",
    "            }\n",
    "        )\n",
    "\n",
    "import fcntl  # Use portalocker for Windows\n",
    "def write_summary(df_path=\"summary.csv\", df: pd.DataFrame=None):\n",
    "    \"\"\"Safely appends a DataFrame to summary.csv using file locking.\"\"\"\n",
    "    with open(df_path, \"a\") as f:\n",
    "        try:\n",
    "            fcntl.flock(f, fcntl.LOCK_EX)  # Lock the file for exclusive access\n",
    "            df.to_csv(f, header=True, index=False)  # Append new rows without headers\n",
    "            f.flush()  # Ensure data is written immediately\n",
    "        finally:\n",
    "            fcntl.flock(f, fcntl.LOCK_UN)  # Unlock the file\n",
    "\n",
    "df_path=\"summary.csv\"\n",
    "write_summary(df=result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top_k': [5, 10, 20, 30]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lines_with_nodes(self, *nodes) -> List[Dict]:\n",
    "    node_lines = [{\n",
    "        \"node_line_name\": \"post_retrieve_node_line\",\n",
    "        \"nodes\": list(node_group)\n",
    "    } for node_group in zip(*nodes)]\n",
    "    return node_lines\n",
    "\n",
    "# test create_lines_with_nodes\n",
    "nodes = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-crafted YAML Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded YAML Data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vectordb': [{'name': 'chroma_mpnet',\n",
       "   'db_type': 'chroma',\n",
       "   'client_type': 'persistent',\n",
       "   'collection_name': 'huggingface_all_mpnet_base_v2',\n",
       "   'embedding_model': 'huggingface_all_mpnet_base_v2',\n",
       "   'path': '/home/lyb/RAG/experiments/chroma_mpnet'}],\n",
       " 'node_lines': [{'node_line_name': 'retrieve_node_line',\n",
       "   'nodes': [{'node_type': 'retrieval',\n",
       "     'strategy': {'metrics': ['retrieval_f1',\n",
       "       'retrieval_recall',\n",
       "       'retrieval_precision']},\n",
       "     'top_k': 3,\n",
       "     'modules': [{'module_type': 'bm25'},\n",
       "      {'module_type': 'vectordb', 'vectordb': 'chroma_mpnet'},\n",
       "      {'module_type': 'hybrid_rrf',\n",
       "       'target_modules': \"('bm25', 'vectordb')\",\n",
       "       'rrf_k': [3, 5, 10]},\n",
       "      {'module_type': 'hybrid_cc',\n",
       "       'target_modules': \"('bm25', 'vectordb')\",\n",
       "       'weights': '(0.5, 0.5)'}]}]},\n",
       "  {'node_line_name': 'post_retrieve_node_line',\n",
       "   'nodes': [{'node_type': 'prompt_maker',\n",
       "     'strategy': {'metrics': ['meteor', 'rouge', 'bert_score']},\n",
       "     'modules': [{'module_type': 'fstring',\n",
       "       'prompt': 'Read the passages and answer the given question. \\n Question: {query} \\n Passage: {retrieved_contents} \\n Answer : '}]},\n",
       "    {'node_type': 'generator',\n",
       "     'strategy': {'metrics': ['meteor', 'rouge', 'bert_score']},\n",
       "     'modules': [{'module_type': 'llama_index_llm',\n",
       "       'llm': 'ollama',\n",
       "       'model': 'llama3',\n",
       "       'temperature': 0.5,\n",
       "       'batch': 1}]}]}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"../ollama_config.yaml\", \"r\") as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "print(\"Loaded YAML Data:\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
