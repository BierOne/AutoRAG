vectordb:
  method: [chroma, couchbase, milvus, pinecone, qdrant, weaviate]
  collection_name: [huggingface_all_mpnet_base_v2]
  embedding_model: [huggingface_all_mpnet_base_v2]
  embedding_batch: [100, 200]
  similarity_metric: [cosine, euclidean, ip]
  ingest_batch: [100, 200]
  milvus:
    index_type: [IVF_FLAT, ScaNN, IVF_SQ8, SCANN, GPU_CAGRA, GPU_IVF_FLAT] # https://milvus.io/docs/index.md?tab=floating


prompt_maker:
  method: [fstring, window_replacement, long_context_reorder]
  prompt:
    set: ["Tell me something about the question: {query} \n\n {retrieved_contents}",
          "Question: {query} \n Something to read: {retrieved_contents} \n What's your answer?"]


generator: # api_base, api_key
  method: [LlamaIndexLLM, OpenAILLM, Vllm, VllmAPI]
  temperature: [0.5, 1.0, 1.5] # The temperature of the sampling. Higher temperature means more randomness.
  batch: [16, 32]
  max_token: 512
  LlamaIndexLLM:
    llm: [openai, openailike, mock, bedrock, huggingfacellm, ollama]
    # model should be with condition (gpt-3.5-turbo* should use llm:openai, mistralai* should use openailike)
    model: [gpt-3.5-turbo-16k, gpt-3.5-turbo-1106, llama3, mistralai/Mistral-7B-Instruct-v0.2]
    torch_dtype: "float16"
  OpenAILLM: # static params: truncate, api_key
    llm: [gpt-3.5-turbo, gpt-4-turbo-2024-04-09] # A model name for openai.
  Vllm:
    llm: [mistralai/Mistral-7B-Instruct-v0.2, facebook/opt-125m]
    top_p: (0, 1.0) # Float that controls the cumulative probability of the top tokens to consider. Must be in (0, 1]. Set to 1 to consider all tokens.
  VllmAPI: # static params: uri
    llm: [Qwen/Qwen2.5-14B-Instruct-AWQ]
