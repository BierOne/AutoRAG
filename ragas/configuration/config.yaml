vectordb:
  method: [chroma]
#  method: [chroma, couchbase, milvus, pinecone, qdrant, weaviate]
  collection_name: [huggingface_all_mpnet_base_v2]
  embedding_model: [huggingface_all_mpnet_base_v2]
  embedding_batch: [100, 200]
  similarity_metric: [cosine, l2, ip]
  couchbase:
    ingest_batch: [100, 200]
  pinecone:
    ingest_batch: [100, 200]
  milvus:
    ingest_batch: [100, 200]
    index_type: [IVF_FLAT, ScaNN, IVF_SQ8, SCANN, GPU_CAGRA, GPU_IVF_FLAT] # https://milvus.io/docs/index.md?tab=floating

#query_expansion:
#  method: [pass_query_expansion, query_decompose, hyde, multi_query_expansion]
#  #  TODO: inherit from general generator (cond: hyde, multi, decompose)
#  query_decompose:
#    prompt: ["""Decompose a question in self-contained sub-questions. Use \"The question needs no decomposition\" when no decomposition is needed.
#
#    Example 1:
#
#    Question: Is Hamlet more common on IMDB than Comedy of Errors?
#    Decompositions:
#    1: How many listings of Hamlet are there on IMDB?
#    2: How many listing of Comedy of Errors is there on IMDB?
#
#    Example 2:
#
#    Question: Are birds important to badminton?
#
#    Decompositions:
#    The question needs no decomposition
#
#    Example 3:
#
#    Question: Is it legal for a licensed child driving Mercedes-Benz to be employed in US?
#
#    Decompositions:
#    1: What is the minimum driving age in the US?
#    2: What is the minimum age for someone to be employed in the US?
#
#    Example 4:
#
#    Question: Are all cucumbers the same texture?
#
#    Decompositions:
#    The question needs no decomposition
#
#    Example 5:
#
#    Question: Hydrogen's atomic number squared exceeds number of Spice Girls?
#
#    Decompositions:
#    1: What is the atomic number of hydrogen?
#    2: How many Spice Girls are there?
#
#    Example 6:
#
#    Question: {question}
#
#    Decompositions:
#    """]
#  hyde:
#    prompt: [ "Please write a passage to answer the question" ]
#  multi_query_expansion:
#    prompt: [ """You are an AI language model assistant.
#    Your task is to generate 3 different versions of the given user
#    question to retrieve relevant documents from a vector  database.
#    By generating multiple perspectives on the user question,
#    your goal is to help the user overcome some of the limitations
#    of distance-based similarity search. Provide these alternative
#    questions separated by newlines. Original question: {query}""" ]


retrieval:
  method: [hybrid_rrf] # hybrid_rrf
  top_k: [5, 10] # top ‘k’ results to be retrieved from corpus.
  bm25:
    # Korean: ko_kiwi | ko_kkma | ko_okt, Japanese: sudachipy, English: space | porter_stemmer
    bm25_tokenizer: [ porter_stemmer, space]
  vectordb:
    embedding_batch: [256] # The number of queries to be processed in parallel.
  hybrid_cc:
    normalize_method: [tmm, mm, z, dbsf] # The normalization method that you want to use.
    weight: (0.0, 1.0) # If the weight is 1.0, it means the weight to the semantic module will be 1.0 and weight to the lexical module will be 0.0.
    lexical_theoretical_min_value: 0  # This value used by tmm normalization method.
    semantic_theoretical_min_value: -1 # This value used by tmm normalization method.
  hybrid_rrf:
    weight: (4, 80) # It was originally rrf_k value.
#
#passageaugmenter:
#  method: [pass_passage_augmenter, prev_next_augmenter]
#  top_k: [5, 10, 20, 30] # top ‘k’ results to be retrieved from corpus. (less or equal "top_k" in retrival)
#  prev_next_augmenter:
#    mode: [prev, next, both] # prev: add passages before the retrieved passage; next: add passages after the retrieved passage; both: add passages before and after the retrieved passage.
#    num_passages: [1, 2, 3] # The number of passages to be added before or after the retrieved passage.
#    embedding_model: #  TODO: inherit from general embedding model (cond: hyde, multi, decompose)
#
#
#reranker:
#  method: [upr, tart, monot5, koreranker, cohere_reranker, rankgpt, jina_reranker, colbert_reranker, sentence_transformer_reranker, flag_embedding_reranker, flag_embedding_llm_reranker, time_reranker, openvino_reranker, voyageai_reranker, mixedbreadai_reranker, flashrank_reranker]
#  speed_threshold: 10
#
#passage_filter:
#  metrics: [ retrieval_f1, retrieval_recall, retrieval_precision ]
#  method: [similarity_threshold_cutoff, similarity_percentile_cutoff, recency_filter, threshold_cutoff, percentile_cutoff]
#  speed_threshold: 5
#
#passage_compressor:
#    metrics: [retrieval_token_f1, retrieval_token_recall, retrieval_token_precision]
#    method: [tree_summarize, refine, longllmlingua]
#    speed_threshold: 10
#
#
prompt_maker:
  method: [fstring, long_context_reorder] # window_replacement
  prompt: ["Tell me something about the question: {query} \n {retrieved_contents}",
          "Question: {query} \n Something to read: {retrieved_contents} \n What's your answer?"]


generator: # api_base, api_key
  method: [LlamaIndexLLM,]
  temperature: [0.1, 0.5] # The temperature of the sampling. Higher temperature means more randomness.
  batch: [1]
  max_token: 512
  LlamaIndexLLM:
    # TODO: add available list
    llm: [ollama]
    model: [deepseek-r1:7b] # llama3 qwen deepseek-r1:7b
#    llm: [openai, openailike, mock, bedrock, huggingfacellm, ollama]
#     model should be with condition (gpt-3.5-turbo* should use llm:openai, mistralai* should use openailike)
#    model: [gpt-3.5-turbo-16k, gpt-3.5-turbo-1106, llama3, mistralai/Mistral-7B-Instruct-v0.2]
    torch_dtype: "float16"
  OpenAILLM: # static params: truncate, api_key
    llm: [gpt-3.5-turbo, gpt-4-turbo-2024-04-09] # A model name for openai.
  Vllm:
    llm: [mistralai/Mistral-7B-Instruct-v0.2, facebook/opt-125m]
    top_p: (0, 1.0) # Float that controls the cumulative probability of the top tokens to consider. Must be in (0, 1]. Set to 1 to consider all tokens.
  VllmAPI: # static params: uri
    llm: [Qwen/Qwen2.5-14B-Instruct-AWQ]
